{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Running cells with 'venv (Python 3.12.12)' requires the notebook package.\n",
      "\u001b[1;31mInstall 'jupyter and notebook' into the Python environment. \n",
      "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
      "\u001b[1;31mor\n",
      "\u001b[1;31mconda install jupyter notebook -U'\n",
      "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "from typing import TypedDict, Dict\n",
    "from langgraph.graph import StateGraph, START,END\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # take environment variables from .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    temperature = 0,\n",
    "    model_name = \"llama-3.3-70b-versatile\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"What is LangGraph?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AIMessage(content=\"LangGraph is an open-source, multilingual, large language model developed by the LangGraph team. It's designed to process and generate human-like language, similar to other popular language models like LLaMA or BERT.\\n\\nLangGraph is unique in that it's specifically designed to be highly customizable and adaptable to various languages and tasks. The model is trained on a massive dataset of text from the internet, books, and other sources, which allows it to learn patterns and relationships in language.\\n\\nSome of the key features of LangGraph include:\\n\\n1. **Multilingual support**: LangGraph is trained on text data in multiple languages, making it a great option for applications that require language support beyond English.\\n2. **Customizability**: The model can be fine-tuned for specific tasks or domains, allowing developers to adapt it to their particular use case.\\n3. **Large-scale training**: LangGraph is trained on a massive dataset, which enables it to learn complex patterns and relationships in language.\\n4. **Open-source**: The model is open-source, which means that developers can access and modify the code to suit their needs.\\n\\nLangGraph has a wide range of potential applications, including:\\n\\n1. **Language translation**: LangGraph can be used to improve machine translation systems, especially for low-resource languages.\\n2. **Text generation**: The model can be used to generate human-like text, such as articles, stories, or chatbot responses.\\n3. **Sentiment analysis**: LangGraph can be fine-tuned for sentiment analysis tasks, such as determining the emotional tone of text.\\n4. **Question answering**: The model can be used to answer questions based on the text it has been trained on.\\n\\nOverall, LangGraph is a powerful and flexible language model that has the potential to revolutionize the way we interact with language.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 367, 'prompt_tokens': 40, 'total_tokens': 407, 'completion_time': 0.926001859, 'completion_tokens_details': None, 'prompt_time': 0.003211938, 'prompt_tokens_details': None, 'queue_time': 0.080323806, 'total_time': 0.929213797}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_93b5f9e564', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019afc04-b04e-7993-9a8a-398e9f594237-0', usage_metadata={'input_tokens': 40, 'output_tokens': 367, 'total_tokens': 407})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    query : str\n",
    "    category : str\n",
    "    sentiment : str\n",
    "    response : str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize(state: State) -> State:\n",
    "    \"Technical , Billing , General\"\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \" Categorize the following customer query into one of these categories:\" \n",
    "        \" Technical, Billing, General.\\n\\nQuery: {query}\"\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm\n",
    "    category = chain.invoke({\"query\": state[\"query\"]}).content\n",
    "    print(\"category result:\", category)\n",
    "    return {**state, \"category\": category}\n",
    "\n",
    "def analyze_sentiment(state: State) -> State:\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \" Analyze the sentiment of the following customer query\"\n",
    "        \"Response with either 'Positive', 'Negative', or 'Neutral'.\\n\\nQuery: {query}\"\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm\n",
    "    sentiment = chain.invoke({\"query\": state[\"query\"]}).content\n",
    "    print(\"sentiment analysis result:\", sentiment)\n",
    "    return {**state, \"sentiment\": sentiment}\n",
    "\n",
    "\n",
    "def handle_technical(state: State) -> State:\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \" Provide a technical support response for the following query:\\n\\nQuery: {query}\"\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\"query\": state[\"query\"]}).content\n",
    "    print(\"response from technical handler:\", response)\n",
    "    return {**state, \"response\": response}\n",
    "\n",
    "\n",
    "def handle_billing(state: State) -> State:\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \" Provide a billing support response for the following query:\\n\\nQuery: {query}\"\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\"query\": state[\"query\"]}).content\n",
    "    print(\"response from billing handler:\", response)\n",
    "    return {**state, \"response\": response}\n",
    "\n",
    "def handle_general(state: State) -> State:\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \" Provide a general support response for the following query:\\n\\nQuery: {query}\"\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\"query\": state[\"query\"]}).content\n",
    "    print(\"response from general handler:\", response)\n",
    "    return {**state, \"response\": response}\n",
    "\n",
    "\n",
    "def escalate(state: State) -> State:\n",
    "    print(\"escalating query due to negative sentiment.\")\n",
    "    return {**state, \"response\": \"Your query has been escalated to a human agent beacause of its Negative sentiment.\"}\n",
    "\n",
    "\n",
    "def route_query(state: State) -> State:\n",
    "    if state[\"sentiment\"] == \"Negative\":\n",
    "        return \"escalate\"\n",
    "    if state[\"category\"] == \"Technical\":\n",
    "        return \"handle_technical\"\n",
    "    if state[\"category\"] == \"Billing\":\n",
    "        return \"handle_billing\"\n",
    "    \n",
    "    return \"handle_general\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "graph = StateGraph(State)\n",
    "graph.add_node(\"categorize\",categorize)\n",
    "graph.add_node(\"analyze_sentiment\",analyze_sentiment)\n",
    "graph.add_node(\"handle_technical\",handle_technical)\n",
    "graph.add_node(\"handle_billing\",handle_billing)\n",
    "graph.add_node(\"handle_general\",handle_general)\n",
    "graph.add_node(\"escalate\",escalate)\n",
    "\n",
    "graph.add_edge(START,\"categorize\")\n",
    "graph.add_edge(\"categorize\",\"analyze_sentiment\")\n",
    "graph.add_conditional_edges(\"analyze_sentiment\",\n",
    "                            route_query,\n",
    "                            {\n",
    "                                \"handle_technical\": \"handle_technical\",\n",
    "                                \"handle_billing\": \"handle_billing\",\n",
    "                                \"handle_general\": \"handle_general\", # \"name\" : \"node_name\"\n",
    "                                \"escalate\": \"escalate\" \n",
    "                            }\n",
    "                           )\n",
    "graph.add_edge(\"handle_technical\",END)\n",
    "graph.add_edge(\"handle_billing\",END)\n",
    "graph.add_edge(\"handle_general\",END)\n",
    "graph.add_edge(\"escalate\",END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    Image(\n",
    "        app.get_graph().draw_mermaid_png(\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_customer_support_agent(query: str) -> Dict[str, str]:\n",
    "    initial_state: State = {\n",
    "        \"query\": query,\n",
    "        \"category\": \"\",\n",
    "        \"sentiment\": \"\",\n",
    "        \"response\": \"\"\n",
    "    }\n",
    "    final_state = app.invoke(initial_state)\n",
    "    return {\n",
    "        \"Category\": final_state[\"category\"],\n",
    "        \"Sentiment\": final_state[\"sentiment\"],\n",
    "        \"Response\": final_state[\"response\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_customer_support_agent(\"I am unable to access my account after the recent update. Can you help?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradio_interface(query: str):\n",
    "    result =  run_customer_support_agent(query)\n",
    "    return (\n",
    "        f\"**Category:** {result['Category']}\\n\\n\"\n",
    "        f\"**Sentiment:** {result['Sentiment']}\\n\\n\"\n",
    "        f\"**Response:** {result['Response']}\\n\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Build The Gradio App\n",
    "gui = gr.Interface(\n",
    "    fn=gradio_interface,\n",
    "    inputs=gr.Textbox(lines=2, label=\"Enter your query here\"),\n",
    "    outputs=gr.Markdown(label=\"Support Agent Response\"),\n",
    "    title=\"Customer Support Agent with LLMs\",\n",
    "    description=\"Enter a customer query to receive a categorized response along with sentiment analysis.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    gui.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
